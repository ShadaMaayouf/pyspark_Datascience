{
 "cells": [
  {
   "cell_type": "raw",
   "id": "15b94051",
   "metadata": {},
   "source": [
    "Name:\n",
    "Matrikelnummer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4b8e40",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1b5c898-12fc-408a-aa1f-2663a09461d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5670741",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:30:38.849452Z",
     "start_time": "2022-05-20T13:30:38.731148Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62da065",
   "metadata": {},
   "source": [
    "# (I can't get no) satisfaction\n",
    "Wir arbeiten weiterhin mit dem Datensatz über die Zufriedenheit von Angestellten einer fiktiven Firma. In dieser Kurseinheit betrachten wir die parallele Verarbeitung mit Spark. \n",
    "\n",
    "*Anmerkung: Der verwendete Datensatz ist natürlich eher klein, so dass normalerweise keine Parallelisierung benötigt wird. In der Kurseinheit dient er aber dazu, die grundlegenden Konzepte von Spark zu vermitteln.*\n",
    "\n",
    "*Wichtige Methoden finden Sie im Foliensatz, es empfiehlt sich jedoch auch, die <a href=\"https://spark.apache.org/docs/latest/api/python/reference/index.html\">pyspark-Doku</a> für die Bearbeitung zu nutzen.*\n",
    "\n",
    "*In dieser Kurseinheit werden u.a. Lambda-Funktionen verwendet. Wenn Sie damit noch nicht vetraut sind, hilft ein Blick in die <a href=\"https://www.w3schools.com/python/python_lambda.asp\">Python-Lambas-Doku</a>.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceb036c",
   "metadata": {},
   "source": [
    "Den Eintrittspunkt in die Funktionalität von Spark stellt der SparkContext dar. Dieser repräsentiert auch die Verbindung zu den Exekutoren. Bei der Erstellung wird außerdem eine Sammlung von Konfigurationsparametern mitgegeben. Wir beschränken uns zunächst auf die Angabe des Masters und einem App-Namen:\n",
    "- **master:** Wir verwenden YARN als Ressourcen-Manager und starten die Anwendung im Client-Modus.\n",
    "- **appName:** Hier vergeben wir einen Namen für unsere Applikation.\n",
    "\n",
    "Da wir später auch mit SparkSQL arbeiten wollen, erstellen wir zudem direkt eine SparkSession. Diese stellt den Eingangspunkt für die SparkSQL-Funktionalität zur Verfügung und beinhaltet auch den SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4db27dc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:30:52.527516Z",
     "start_time": "2022-05-20T13:30:39.959013Z"
    }
   },
   "outputs": [],
   "source": [
    "# SparkSession erstellen\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"My Spark Application\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# SparkContext\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87597377",
   "metadata": {},
   "source": [
    "## RDDs\n",
    "Zunächst betrachten wir die Arbeit mit RDDs. Wie Sie in den Vorlesungsvideos gelernt haben, bilden diese die Grundlage der parallelen Verarbeitung. In der Praxis wird meist nicht direkt mit RDDs, sondern mit einer High-level-Bibliothek wie SparkSQL gearbeitet. Es ist jedoch wichtig, die Konzepte von RDDs zu verstehen, da diese die Basis von Spark darstellen. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0df681",
   "metadata": {},
   "source": [
    "### Eigenschaften von RDDs\n",
    "Betrachten wir zunächst die wichtigsten Eigenschaften von RDDs:\n",
    "- In-Memory-Verarbeitung\n",
    "- Lazy\n",
    "- Partitioniert\n",
    "- Unveränderlich\n",
    "- Lineage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48079e69",
   "metadata": {},
   "source": [
    "#### In-Memory-Verarbeitung\n",
    "Beschreiben Sie die Vorteile der In-Memory-Verarbeitung."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ab0c320",
   "metadata": {},
   "source": [
    "Ihre Antwort:\n",
    "- Schnellere Datenverarbeitung: Durch die Speicherung von Daten im Arbeitsspeicher (In-Memory) können Datenverarbeitungsaufgaben schneller ausgeführt werden als bei herkömmlichen Festplatten-basierten Systemen wie Hadoop.\n",
    "\n",
    "- Effeziente Wiederverwendung von Daten: viele Algorithmen und Abfragen arbeiten iterativ auf demselben Datensatz. Die In-Memory-Verarbeitung ermöglicht eine effiziente Wiederverwendung dieser Daten, da sie zwischen Abfragen im Arbeitsspeicher gespeichert werden können.\n",
    "Fehlertoleranz: RDDs können nach einem Ausfall wiederhergestellt werden. Dies wird erreicht, indem die Abstammungslinie (Lineage) jedes RDDs verfolgt wird.\n",
    "Skalierbarkeit und Prallelität: RDDs können über ein Cluster verteilt und parallel verarbeitet werden, was die Verarbeitung von extrem großen Datenmengen ermöglicht\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f7ab8d",
   "metadata": {},
   "source": [
    "#### Lazy\n",
    "Beschreiben Sie, was es für die Programmierung bedeutet, dass die Verarbeitung lazy ausgeführt wird. Programmieren Sie ein Code-Beispiel Ihrer Wahl, in dem Sie die Auswirkungen der lazy Ausführung demonstrieren. Nennen Sie zudem die wichtigsten Unterschiede zwischen Transformations und Actions."
   ]
  },
  {
   "cell_type": "raw",
   "id": "813acaa4",
   "metadata": {},
   "source": [
    "Auswirkungen der lazy Ausführung:\n",
    "In der Programmierung bedeutet “Lazy-Ausführung” (oder “Lazy-Evaluation”), dass Berechnungen erst dann ausgeführt werden, wenn sie tatsächlich benötigt werden. Dies kann die Effizienz von Programmen verbessern, indem unnötige Berechnungen vermieden und Ressourcen gespart werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec8ca89f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:14:30.836829Z",
     "start_time": "2022-05-18T12:14:30.600665Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Ein gutes Beispiel für Lazy-Ausführung ist die Verwendung von Generatoren in Python. \n",
    "Ein Generator erzeugt Werte “on the fly”, aber nur wenn sie benötigt werden. \n",
    "'''\n",
    "def count_up_to(n):\n",
    "    count = 1\n",
    "    while count <= n:\n",
    "        yield count\n",
    "        count += 1\n",
    "\n",
    "# Erstellen des Generatord, aber es wird noch nichts berechnet\n",
    "numbers = count_up_to(5)\n",
    "\n",
    "# Die Zahlen werden erst generiert, wenn wir durch sie iterieren\n",
    "for number in numbers:\n",
    "    print(number)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b3b41bc1",
   "metadata": {},
   "source": [
    "Wichtigste Unterschiede zwischen Transformations und Actions:\n",
    "\n",
    "In Apache Spark sind Transformationen und Aktionen zwei Arten von Operationen, die auf Daten angewendet werden können. Hier sind die wichtigsten Unterschiede:\n",
    "\n",
    "- **Transformationen** erstellen ein neues Dataset aus einem vorhandenen Dataset. Beispiele für Transformationen sind `map`, `filter` und `reduceByKey`. Transformationen in Spark sind \"lazy\", was bedeutet, dass sie erst ausgeführt werden, wenn eine Aktion aufgerufen wird. Sie führen also keine Berechnungen durch, wenn sie definiert werden, sondern erstellen lediglich einen Ausführungsplan.\n",
    "\n",
    "- **Aktionen** hingegen liefern einen Wert an den Treiberprogramm zurück oder schreiben Daten in ein externes Speichersystem. Aktionen lösen die tatsächliche Berechnung aus, die durch die Transformationen definiert wurde. Beispiele für Aktionen sind `count`, `first`, `take`, `collect` und `save`.\n",
    "\n",
    "Zusammengefasst, Transformationen erstellen einen Ausführungsplan, aber berechnen nichts, bis eine Aktion aufgerufen wird. Aktionen lösen die Berechnung aus und geben das Ergebnis zurück. Dieses Konzept ermöglicht es Spark, seine Berechnungen zu optimieren und effizient über große Datenmengen hinweg durchzuführen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b00336b",
   "metadata": {},
   "source": [
    "#### Partitioniert\n",
    "Erstellen Sie aus der vorgegebenen Liste (`data`) ein RDD namens `rdd_part` mit drei Partitionen. Kontrollieren Sie nach der Erstellung, dass die korrekte Anzahl Partitionen erzeugt wurde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fe841dcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:14:57.413324Z",
     "start_time": "2022-05-18T12:14:57.409659Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [1 for i in range (10)] + [0 for i in range (5)]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ca697ec9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:15:19.658042Z",
     "start_time": "2022-05-18T12:15:19.651377Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD erstellen\n",
    "rdd_part =  sc.parallelize(data, 3)\n",
    "rdd_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a146eee4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T12:05:34.579627Z",
     "start_time": "2022-05-16T12:05:34.569123Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung\n",
    "num_partitions = rdd_part.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b0f287ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:15:23.007815Z",
     "start_time": "2022-05-18T12:15:22.996956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das RDD hat 3 Partitionen.\n"
     ]
    }
   ],
   "source": [
    "# Anzahl Partitionen anzeigen\n",
    "'''\n",
    "n diesem Code wird ein RDD namens rdd_part erstellt, das die Daten aus der Liste data enthält und in drei Partitionen aufgeteilt ist. Die Anzahl der Partitionen wird dann mit getNumPartitions überprüft und ausgegeben.'''\n",
    "\n",
    "print(f\"Das RDD hat {num_partitions} Partitionen.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3821b4d0",
   "metadata": {},
   "source": [
    "Im Folgenden schauen wir uns die Verteilung der Daten auf die unterschiedlichen Partitionen an.\n",
    "\n",
    "> **Achtung:** Die Anwendung von `collect()` in Kombination mit `glom()` ist nur für kleine Datensätze geeignet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b296885",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:15:31.503029Z",
     "start_time": "2022-05-18T12:15:31.234810Z"
    }
   },
   "outputs": [],
   "source": [
    "rdd_part.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c06286",
   "metadata": {},
   "source": [
    "Wie wir sehen können, bestehen zwei Partitionen aus 1en und eine aus 0en. Filtern Sie das RDD nun so, dass nur die Werte größer 0 übrig bleiben. Erstellen Sie hierfür ein neues RDD namens `rdd_filtered`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "430cbab8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:15:38.268146Z",
     "start_time": "2022-05-18T12:15:38.263951Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung\n",
    "rdd_filtered = rdd_part.filter(lambda x: x > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17b3993",
   "metadata": {},
   "source": [
    "Wenn wir uns nun die Verteilung der Daten auf die Partitionen ansehen, sehen wir, dass wir eine leere Partition erzeugt haben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96c007e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:15:42.674871Z",
     "start_time": "2022-05-18T12:15:42.499927Z"
    }
   },
   "outputs": [],
   "source": [
    "rdd_filtered.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b22c444",
   "metadata": {},
   "source": [
    "Beschreiben Sie die Nachteile von leeren Partitionen. Wie kann dieses Problem gelöst werden (Stichwort *Shuffling*) und warum sollte Shuffling normalerweise vermieden werden?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ae0d5997",
   "metadata": {},
   "source": [
    "Ihre Antwort:\n",
    "\n",
    "Leere Partitionen in einem RDD können zu mehreren Problemen führen:\n",
    "\n",
    "1. **Ressourcenverschwendung**: Jede Partition wird von einem separaten Task verarbeitet. Wenn eine Partition leer ist, wird ein Task verschwendet, da es nichts zu verarbeiten gibt.\n",
    "2. **Ungleichmäßige Lastverteilung**: Wenn einige Partitionen viele Daten enthalten und andere leer sind, kann dies zu einer ungleichmäßigen Lastverteilung führen. Einige Tasks können sehr lange dauern, während andere schnell abgeschlossen sind.\n",
    "\n",
    "Um das Problem leerer Partitionen zu lösen, kann man das RDD neu partitionieren, um die Daten gleichmäßiger zu verteilen. Dieser Prozess wird als **Shuffling** bezeichnet. Mit der `repartition()` Funktion in Spark kann man die Anzahl der Partitionen in einem RDD ändern:\n",
    "\n",
    "```python\n",
    "rdd_repartitioned = rdd_filtered.repartition(10)\n",
    "```\n",
    "\n",
    "Obwohl Shuffling das Problem leerer Partitionen lösen kann, sollte es normalerweise vermieden werden, da es mit hohen Kosten verbunden ist:\n",
    "\n",
    "1. **Hoher Netzwerkverkehr**: Beim Shuffling werden alle Daten über das Netzwerk gesendet, was zu hohem Netzwerkverkehr führen kann.\n",
    "2. **Hoher Disk I/O**: Die Daten müssen auf die Festplatte geschrieben und von dort gelesen werden, was zu hohem Disk I/O führt.\n",
    "3. **Hoher CPU-Verbrauch**: Das Sortieren und Aggregieren der Daten erfordert CPU-Ressourcen.\n",
    "\n",
    "Daher sollte Shuffling nur dann verwendet werden, wenn es unbedingt notwendig ist, z.B. wenn die Vorteile der Neuverteilung der Daten die Kosten des Shufflings überwiegen. Es ist immer eine gute Praxis, die Daten so früh wie möglich im Verarbeitungsprozess zu partitionieren, um die Notwendigkeit von Shuffling zu minimieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e85b69c",
   "metadata": {},
   "source": [
    "#### Unveränderlich und Lineage\n",
    "Warum sind RDDs unveränderlich und warum kennt jedes RDD seine Abstammung (Lineage)?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2dfb6147",
   "metadata": {},
   "source": [
    "Ihre Antwort:\n",
    "Resilient Distributed Datasets (RDDs) sind unveränderlich, was bedeutet, dass sie nach ihrer Erstellung nicht mehr verändert werden können. Dies hat mehrere Vorteile:\n",
    "\n",
    "1. **Fehlerbehandlung**: Da RDDs unveränderlich sind, können sie bei einem Fehler einfach neu erstellt werden. Dies macht das System widerstandsfähiger gegen Ausfälle.\n",
    "2. **Parallelisierung**: Unveränderlichkeit erleichtert die Parallelisierung von Operationen, da es keine Synchronisationsprobleme gibt, die normalerweise bei veränderlichen Daten auftreten.\n",
    "3. **Effizienz**: Da RDDs unveränderlich sind, können sie effizient auf mehreren Knoten verteilt und zwischengespeichert werden, was die Verarbeitungsgeschwindigkeit verbessert.\n",
    "\n",
    "Jedes RDD kennt seine Abstammung oder \"Lineage\", was bedeutet, dass es die Reihe von Transformationen kennt, die zu seiner Erstellung geführt haben. Dies hat auch mehrere Vorteile:\n",
    "\n",
    "1. **Fehlerbehandlung**: Wenn ein RDD verloren geht, kann es anhand seiner Lineage neu erstellt werden. Dies macht das System widerstandsfähiger gegen Ausfälle.\n",
    "2. **Optimierung**: Die Kenntnis der Lineage ermöglicht es Spark, die Verarbeitung zu optimieren. Zum Beispiel kann Spark entscheiden, einige Operationen zusammenzufassen oder die Reihenfolge der Operationen zu ändern, um die Effizienz zu verbessern.\n",
    "\n",
    "Zusammenfassend lässt sich sagen, dass die Unveränderlichkeit und die Kenntnis der Lineage zwei der Schlüsseleigenschaften von RDDs sind, die Spark seine Robustheit und Effizienz verleihen. Sie ermöglichen eine effiziente verteilte Verarbeitung und eine robuste Fehlerbehandlung."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dbe486",
   "metadata": {},
   "source": [
    "### Daten laden\n",
    "Laden Sie den Datensatz `employees_satisfaction_transformed.csv` in ein RDD. Denken Sie daran, dass die Daten standardmäßig aus dem HDFS geladen werden. Aus diesem Grund müssen die Daten zunächst ins HDFS geladen werden. Verwenden Sie hierzu das Notebook `hdfs-upload.ipynb`, das wir Ihnen in Moodle zur Verfügung gestellt haben."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f1b5ee",
   "metadata": {},
   "source": [
    "Laden Sie anschließend die Daten in ein RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "25823422-5faa-49e0-b170-035f47ff5e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "./employees_satisfaction_transformed.csv MapPartitionsRDD[2] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_df = sc.textFile(\"./employees_satisfaction_transformed.csv\")\n",
    "rdd_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7854a817",
   "metadata": {},
   "source": [
    "### Erste Datenexploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0281400",
   "metadata": {},
   "source": [
    "Lassen Sie sich dann die ersten zehn Zeilen des RDDs anzeigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b104f7fd-f968-44b2-a43d-25aa44601656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',age,department,education,recruitment_type,job_level,rating,awards,certifications,salary,gender,entry_date,satisfied',\n",
       " '0,28,HR,Postgraduate,Referral,5,2.0,1,0,78075.0,Male,2019-02-01,1',\n",
       " '1,50,Technology,Postgraduate,Recruitment Agency,3,5.0,2,1,38177.1,Male,2017-01-17,0',\n",
       " '2,43,Technology,Undergraduate,Referral,4,1.0,2,0,59143.5,Female,2012-08-27,1',\n",
       " '3,44,Sales,Postgraduate,On-Campus,2,3.0,0,0,26824.5,Female,2017-07-25,1',\n",
       " '4,33,HR,Undergraduate,Recruitment Agency,2,1.0,5,0,26824.5,Male,2019-05-17,1',\n",
       " '5,40,Purchasing,Undergraduate,Walk-in,3,3.0,7,1,38177.1,Male,2004-04-22,1',\n",
       " '6,26,Purchasing,Undergraduate,Referral,5,5.0,2,0,78075.0,Male,2019-12-10,1',\n",
       " '7,25,Technology,Undergraduate,Recruitment Agency,1,1.0,4,0,21668.4,Female,2017-03-18,0',\n",
       " '8,35,HR,Postgraduate,Referral,3,4.0,0,0,38177.1,Female,2015-04-02,1']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_df.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d99e5f",
   "metadata": {},
   "source": [
    "Die erste Zeile entspricht dem Header. Entfernen Sie diese aus dem Datensatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37fcb216",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:16:55.070359Z",
     "start_time": "2022-05-18T12:16:54.946567Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung\n",
    "header = rdd_df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65241bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_no_header = rdd_df.filter(lambda line: line != header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbcf396e-2ded-4d96-9608-6692da59746b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0,28,HR,Postgraduate,Referral,5,2.0,1,0,78075.0,Male,2019-02-01,1',\n",
       " '1,50,Technology,Postgraduate,Recruitment Agency,3,5.0,2,1,38177.1,Male,2017-01-17,0',\n",
       " '2,43,Technology,Undergraduate,Referral,4,1.0,2,0,59143.5,Female,2012-08-27,1',\n",
       " '3,44,Sales,Postgraduate,On-Campus,2,3.0,0,0,26824.5,Female,2017-07-25,1',\n",
       " '4,33,HR,Undergraduate,Recruitment Agency,2,1.0,5,0,26824.5,Male,2019-05-17,1']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_no_header.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f30438",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T12:23:31.207384Z",
     "start_time": "2022-05-16T12:23:31.090921Z"
    }
   },
   "source": [
    "Zählen Sie nun die Anzahl der Angestellten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c2ff2a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:17:03.912327Z",
     "start_time": "2022-05-18T12:17:03.303209Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der Angestellten:  498\n"
     ]
    }
   ],
   "source": [
    "# Ihre Lösung\n",
    "num_employees = rdd_no_header.count()\n",
    "print(\"Anzahl der Angestellten: \", num_employees)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c66d9c",
   "metadata": {},
   "source": [
    "### Datenverarbeitung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bc40b9",
   "metadata": {},
   "source": [
    "Zählen Sie nun die Angestellten pro Department."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5499b286",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "*Sollten Sie keinen Lösungsansatz haben, finden Sie unter dem Button \"Show Solution\" die Beschreibung der notwendigen Verarbeitungs-Schritte.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f383a4",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "1. Aktuell ist jede Zeile des RDD ein String, in dem die einzelnen Felder durch Komma getrennt sind. Dieser String muss zunächst gesplittet werden.\n",
    "2. Im nächsten Schritt müssen Key-Value-Paare gebildet werden. Der Key ist das Department und der Wert ist 1.\n",
    "3. Dann müssen pro Key alle Werte addiert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71226012",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "*Wenn Sie Probleme mit der konkreten Umsetzung der Aufgabe haben, finden Sie unter dem Button \"Show Solution\" Hinweise zur Programmierung.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffd24fd",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "- Das Splitten der einzelnen Funktionen erfolgt mit `split(\",\")`.\n",
    "- Anschließend liegen die Daten in einer Liste vor. Das dritte Element (`line[2]`) entspricht dem Department.\n",
    "- Mithilfe der Funktion `reducyByKey()` können die Werte pro Key addiert werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dda5795c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0',\n",
       "  '28',\n",
       "  'HR',\n",
       "  'Postgraduate',\n",
       "  'Referral',\n",
       "  '5',\n",
       "  '2.0',\n",
       "  '1',\n",
       "  '0',\n",
       "  '78075.0',\n",
       "  'Male',\n",
       "  '2019-02-01',\n",
       "  '1'],\n",
       " ['1',\n",
       "  '50',\n",
       "  'Technology',\n",
       "  'Postgraduate',\n",
       "  'Recruitment Agency',\n",
       "  '3',\n",
       "  '5.0',\n",
       "  '2',\n",
       "  '1',\n",
       "  '38177.1',\n",
       "  'Male',\n",
       "  '2017-01-17',\n",
       "  '0'],\n",
       " ['2',\n",
       "  '43',\n",
       "  'Technology',\n",
       "  'Undergraduate',\n",
       "  'Referral',\n",
       "  '4',\n",
       "  '1.0',\n",
       "  '2',\n",
       "  '0',\n",
       "  '59143.5',\n",
       "  'Female',\n",
       "  '2012-08-27',\n",
       "  '1'],\n",
       " ['3',\n",
       "  '44',\n",
       "  'Sales',\n",
       "  'Postgraduate',\n",
       "  'On-Campus',\n",
       "  '2',\n",
       "  '3.0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '26824.5',\n",
       "  'Female',\n",
       "  '2017-07-25',\n",
       "  '1'],\n",
       " ['4',\n",
       "  '33',\n",
       "  'HR',\n",
       "  'Undergraduate',\n",
       "  'Recruitment Agency',\n",
       "  '2',\n",
       "  '1.0',\n",
       "  '5',\n",
       "  '0',\n",
       "  '26824.5',\n",
       "  'Male',\n",
       "  '2019-05-17',\n",
       "  '1']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ihre Lösung\n",
    "#Splitting\n",
    "rdd_split = rdd_no_header.map(lambda line: line.split(','))\n",
    "rdd_split.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "317eb367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('HR', 1), ('Technology', 1), ('Technology', 1), ('Sales', 1), ('HR', 1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Key-Value-Paare erstellen:\n",
    "rdd_kv = rdd_split.map(lambda line: (line[2], 1))  # line[2] ist das Department\n",
    "rdd_kv.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "007612dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Werte pro Key addieren:\n",
    "rdd_counts = rdd_kv.reduceByKey(lambda a, b: a + b)\n",
    "employee_per_department = rdd_counts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7762ea15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Department: HR, Anzahl der Angestellten: 106\n",
      "Department: Technology, Anzahl der Angestellten: 98\n",
      "Department: Sales, Anzahl der Angestellten: 87\n",
      "Department: Purchasing, Anzahl der Angestellten: 114\n",
      "Department: Marketing, Anzahl der Angestellten: 93\n"
     ]
    }
   ],
   "source": [
    "for department, count in employee_per_department:\n",
    "    print(f\"Department: {department}, Anzahl der Angestellten: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b975a2e",
   "metadata": {},
   "source": [
    "Die Arbeit mit RDDs ist offensichtlich recht mühsam, weshalb in der Praxis häufig eine High-level API wie SparkSQL verwendet wird. Diese schauen wir uns im Folgenden an."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d926af8d",
   "metadata": {},
   "source": [
    "## DataFrames\n",
    "Im Folgenden arbeiten wir mit der High-level API SparkSQL.\n",
    "\n",
    "*Vorsicht: DataFrames sind die zentrale Abstraktion der SparkSQL-Bibiliothek. Die Bibliothek bietet zwei praktisch gleichwertige APIs, um mit DataFrames zu arbeiten: Die **DataFrame API** und die **SQL API**. Wir arbeiten zunächst mit der DataFrame API.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33b8b35",
   "metadata": {},
   "source": [
    "### Daten laden\n",
    "Laden Sie die Daten aus der Datei `employees_satisfaction_transformed.csv` in ein DataFrame. Die Spaltenbezeichnungen sollen dabei im Header stehen und nicht Teil der Daten sein, nutzen Sie außerdem die automatische Schemaerkennung von Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c317b86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:16:45.740759Z",
     "start_time": "2022-05-18T12:16:45.691953Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung\n",
    "df = spark.read.csv(\"./employees_satisfaction_transformed.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2890497-e61f-48f9-81ee-7333cbc6f4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: int, age: int, department: string, education: string, recruitment_type: string, job_level: int, rating: string, awards: int, certifications: int, salary: double, gender: string, entry_date: date, satisfied: int]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3af8746",
   "metadata": {},
   "source": [
    "Lassen Sie sich das Schema ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2fa1cc9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:05.871051Z",
     "start_time": "2022-05-20T13:31:05.855291Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- recruitment_type: string (nullable = true)\n",
      " |-- job_level: integer (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- awards: integer (nullable = true)\n",
      " |-- certifications: integer (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- entry_date: date (nullable = true)\n",
      " |-- satisfied: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ihre Lösung\n",
    "# Schema ausgeben\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef82feb5",
   "metadata": {},
   "source": [
    "Lassen Sie sich die Daten anzeigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d766b363",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:16:49.061163Z",
     "start_time": "2022-05-18T12:16:48.403315Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----------+-------------+------------------+---------+------+------+--------------+-------+------+----------+---------+\n",
      "|_c0|age|department|    education|  recruitment_type|job_level|rating|awards|certifications| salary|gender|entry_date|satisfied|\n",
      "+---+---+----------+-------------+------------------+---------+------+------+--------------+-------+------+----------+---------+\n",
      "|  0| 28|        HR| Postgraduate|          Referral|        5|   2.0|     1|             0|78075.0|  Male|2019-02-01|        1|\n",
      "|  1| 50|Technology| Postgraduate|Recruitment Agency|        3|   5.0|     2|             1|38177.1|  Male|2017-01-17|        0|\n",
      "|  2| 43|Technology|Undergraduate|          Referral|        4|   1.0|     2|             0|59143.5|Female|2012-08-27|        1|\n",
      "|  3| 44|     Sales| Postgraduate|         On-Campus|        2|   3.0|     0|             0|26824.5|Female|2017-07-25|        1|\n",
      "|  4| 33|        HR|Undergraduate|Recruitment Agency|        2|   1.0|     5|             0|26824.5|  Male|2019-05-17|        1|\n",
      "|  5| 40|Purchasing|Undergraduate|           Walk-in|        3|   3.0|     7|             1|38177.1|  Male|2004-04-22|        1|\n",
      "|  6| 26|Purchasing|Undergraduate|          Referral|        5|   5.0|     2|             0|78075.0|  Male|2019-12-10|        1|\n",
      "|  7| 25|Technology|Undergraduate|Recruitment Agency|        1|   1.0|     4|             0|21668.4|Female|2017-03-18|        0|\n",
      "|  8| 35|        HR| Postgraduate|          Referral|        3|   4.0|     0|             0|38177.1|Female|2015-04-02|        1|\n",
      "|  9| 45|Technology| Postgraduate|          Referral|        3|   3.0|     9|             0|38177.1|Female|2004-03-19|        0|\n",
      "+---+---+----------+-------------+------------------+---------+------+------+--------------+-------+------+----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ihre Lösung\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab027004",
   "metadata": {},
   "source": [
    "Geben Sie die Anzahl Angestellter aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4c639af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:09.506780Z",
     "start_time": "2022-05-20T13:31:08.979554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Anzahl der Angestellten ist 498.\n"
     ]
    }
   ],
   "source": [
    "# Ihre Lösung\n",
    "# Anzahl der Angestellten ermitteln\n",
    "num_employees = df.count()\n",
    "\n",
    "# Anzahl der Angestellten ausgeben\n",
    "print(f\"Die Anzahl der Angestellten ist {num_employees}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc66789",
   "metadata": {},
   "source": [
    "DataFrames basieren auf RDDs. Deshalb kann auch jedes DataFrame in ein RDD umgewandelt werden.\n",
    "\n",
    "Erstellen Sie aus dem DataFrame der Angestellten ein RDD und lassen Sie sich die ersten fünf Zeilen anzeigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74083f5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:10.911287Z",
     "start_time": "2022-05-20T13:31:10.267345Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=0, age=28, department='HR', education='Postgraduate', recruitment_type='Referral', job_level=5, rating='2.0', awards=1, certifications=0, salary=78075.0, gender='Male', entry_date=datetime.date(2019, 2, 1), satisfied=1),\n",
       " Row(_c0=1, age=50, department='Technology', education='Postgraduate', recruitment_type='Recruitment Agency', job_level=3, rating='5.0', awards=2, certifications=1, salary=38177.1, gender='Male', entry_date=datetime.date(2017, 1, 17), satisfied=0),\n",
       " Row(_c0=2, age=43, department='Technology', education='Undergraduate', recruitment_type='Referral', job_level=4, rating='1.0', awards=2, certifications=0, salary=59143.5, gender='Female', entry_date=datetime.date(2012, 8, 27), satisfied=1),\n",
       " Row(_c0=3, age=44, department='Sales', education='Postgraduate', recruitment_type='On-Campus', job_level=2, rating='3.0', awards=0, certifications=0, salary=26824.5, gender='Female', entry_date=datetime.date(2017, 7, 25), satisfied=1),\n",
       " Row(_c0=4, age=33, department='HR', education='Undergraduate', recruitment_type='Recruitment Agency', job_level=2, rating='1.0', awards=5, certifications=0, salary=26824.5, gender='Male', entry_date=datetime.date(2019, 5, 17), satisfied=1)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ihre Lösung\n",
    "rdd = df.rdd\n",
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b998b3e",
   "metadata": {},
   "source": [
    "Das RDD besteht aus einzelnen Rows. Mittels Index kann auf die einzelnen Rows zugegriffen werden. Der Zugriff auf die Spalten erfolgt entweder auch durch die Angabe des Index oder durch den Spaltennamen.\n",
    "\n",
    "Lassen Sie sich das Gehalt der ersten Row ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b9e0a8f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:12.144784Z",
     "start_time": "2022-05-20T13:31:11.878447Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das Gehalt der ersten Row ist 78075.0.\n"
     ]
    }
   ],
   "source": [
    "# Erste Zeile (Row) des RDD erhalten\n",
    "first_row = rdd.first() #third_row = rdd.take(3)[2]\n",
    "\n",
    "# Gehalt der ersten Zeile ausgeben\n",
    "print(f\"Das Gehalt der ersten Row ist {first_row['salary']}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e5c185",
   "metadata": {},
   "source": [
    "### Verarbeitung\n",
    "*Hinweis: Für die Bearbeitung der folgenden Aufaben ist es hilfreich, sich die `pyspark.sql.functions` anzusehen. Diese sind bereits importiert und können mit `f.<function_name>` verwendet werden.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0134cb",
   "metadata": {},
   "source": [
    "Fügen Sie eine neue Spalte `service_days` hinzu, in der angegeben wird, seit wie vielen Tagen ein\\*e Angestellte\\*r im Unternehmen ist. Verwenden Sie als Stichtag den 01.01.2021 (`comparison`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2fdef45c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:13.934683Z",
     "start_time": "2022-05-20T13:31:13.931821Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'to_date(2021-01-01)'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison = \"2021-01-01\"\n",
    "comparison = f.to_date(f.lit(comparison))\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f5b6240",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:14.474401Z",
     "start_time": "2022-05-20T13:31:14.416892Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----------+-------------+------------------+---------+------+------+--------------+-------+------+----------+---------+------------+\n",
      "|_c0|age|department|    education|  recruitment_type|job_level|rating|awards|certifications| salary|gender|entry_date|satisfied|service_days|\n",
      "+---+---+----------+-------------+------------------+---------+------+------+--------------+-------+------+----------+---------+------------+\n",
      "|  0| 28|        HR| Postgraduate|          Referral|        5|   2.0|     1|             0|78075.0|  Male|2019-02-01|        1|         700|\n",
      "|  1| 50|Technology| Postgraduate|Recruitment Agency|        3|   5.0|     2|             1|38177.1|  Male|2017-01-17|        0|        1445|\n",
      "|  2| 43|Technology|Undergraduate|          Referral|        4|   1.0|     2|             0|59143.5|Female|2012-08-27|        1|        3049|\n",
      "|  3| 44|     Sales| Postgraduate|         On-Campus|        2|   3.0|     0|             0|26824.5|Female|2017-07-25|        1|        1256|\n",
      "|  4| 33|        HR|Undergraduate|Recruitment Agency|        2|   1.0|     5|             0|26824.5|  Male|2019-05-17|        1|         595|\n",
      "|  5| 40|Purchasing|Undergraduate|           Walk-in|        3|   3.0|     7|             1|38177.1|  Male|2004-04-22|        1|        6098|\n",
      "|  6| 26|Purchasing|Undergraduate|          Referral|        5|   5.0|     2|             0|78075.0|  Male|2019-12-10|        1|         388|\n",
      "|  7| 25|Technology|Undergraduate|Recruitment Agency|        1|   1.0|     4|             0|21668.4|Female|2017-03-18|        0|        1385|\n",
      "|  8| 35|        HR| Postgraduate|          Referral|        3|   4.0|     0|             0|38177.1|Female|2015-04-02|        1|        2101|\n",
      "|  9| 45|Technology| Postgraduate|          Referral|        3|   3.0|     9|             0|38177.1|Female|2004-03-19|        0|        6132|\n",
      "| 10| 31| Marketing|Undergraduate|           Walk-in|        4|   4.0|     6|             0|59143.5|  Male|2009-01-24|        1|        4360|\n",
      "| 11| 43|Technology| Postgraduate|Recruitment Agency|        2|   1.0|     9|             1|26824.5|  Male|2016-03-10|        1|        1758|\n",
      "| 12| 28|Technology|Undergraduate|         On-Campus|        3|   4.0|     0|             0|38177.1|Female|2013-04-24|        0|        2809|\n",
      "| 13| 48|Purchasing| Postgraduate|          Referral|        3|   4.0|     8|             0|38177.1|  Male|2010-07-25|        1|        3813|\n",
      "| 14| 52|Purchasing| Postgraduate|Recruitment Agency|        5|   1.0|     7|             0|78075.0|  Male|2018-02-07|        1|        1059|\n",
      "| 15| 50|Purchasing|Undergraduate|Recruitment Agency|        5|   5.0|     6|             0|78075.0|  Male|2014-04-24|        1|        2444|\n",
      "| 16| 34| Marketing| Postgraduate|         On-Campus|        1|   4.0|     9|             0|21668.4|  Male|2014-12-10|        0|        2214|\n",
      "| 17| 24|Purchasing|Undergraduate|Recruitment Agency|        4|   4.0|     6|             0|59143.5|Female|2018-02-18|        1|        1048|\n",
      "| 18| 54|        HR| Postgraduate|         On-Campus|        1|   5.0|     4|             0|21668.4|Female|2014-05-07|        1|        2431|\n",
      "| 19| 25|     Sales|Undergraduate|Recruitment Agency|        5|   4.0|     4|             0|78075.0|  Male|2012-02-15|        1|        3243|\n",
      "+---+---+----------+-------------+------------------+---------+------+------+--------------+-------+------+----------+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ihre Lösung\n",
    "# Neue Spalte hinzufügen\n",
    "df = df.withColumn(\"service_days\", f.datediff(comparison, f.to_date(\"entry_date\")))\n",
    "\n",
    "# DataFrame anzeigen\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fc920c",
   "metadata": {},
   "source": [
    "Lassen Sie sich anschließend das Eintrittsdatum und die Tage im Unternehmen von allen Angestellten anzeigen, die länger als 6000 Tage im Unternehmen sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a2203dc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:16.272880Z",
     "start_time": "2022-05-20T13:31:16.006660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|entry_date|service_days|\n",
      "+----------+------------+\n",
      "|2004-04-22|        6098|\n",
      "|2004-03-19|        6132|\n",
      "|2004-01-16|        6195|\n",
      "|2004-07-13|        6016|\n",
      "|2004-02-24|        6156|\n",
      "|2004-07-15|        6014|\n",
      "|2004-06-20|        6039|\n",
      "|2004-01-20|        6191|\n",
      "|2004-06-01|        6058|\n",
      "|2004-04-19|        6101|\n",
      "|2004-04-13|        6107|\n",
      "|2004-07-05|        6024|\n",
      "|2004-06-08|        6051|\n",
      "|2004-01-20|        6191|\n",
      "|2004-01-22|        6189|\n",
      "|2004-01-05|        6206|\n",
      "|2004-04-17|        6103|\n",
      "|2004-07-05|        6024|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ihre Lösung\n",
    "# DataFrame filtern\n",
    "df_filtered = df.filter(df['service_days'] > 6000)\n",
    "\n",
    "# Eintrittsdatum und Tage im Unternehmen anzeigen\n",
    "df_filtered.select('entry_date', 'service_days').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1c0959",
   "metadata": {},
   "source": [
    "### Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c971c46d",
   "metadata": {},
   "source": [
    "Lassen Sie sich die Departments des Datensatzes anzeigen (jedes nur einmal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ade177b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:23.044052Z",
     "start_time": "2022-05-20T13:31:22.674850Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung\n",
    "# Einzigartige Departments anzeigen\n",
    "df.select(\"Department\").distinct().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f9ad56",
   "metadata": {},
   "source": [
    "Ermitteln Sie (wie eben bei der Arbeit mit RDDs) die Anzahl Angestellte pro Department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869911f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:24.046796Z",
     "start_time": "2022-05-20T13:31:23.715420Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung\n",
    "# Anzahl der Angestellten pro Abteilung ermitteln\n",
    "df_dept_counts = df.groupBy(\"Department\").count()\n",
    "\n",
    "# Ergebnisse ausgeben\n",
    "df_dept_counts.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6c8e3f",
   "metadata": {},
   "source": [
    "Geben Sie durchschnittliche Zufriedenheit aller Personen aus, die in der Abteilung *HR* arbeiten, sowie das Alter der ältesten Person der Abteilung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e624ef5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:26.924816Z",
     "start_time": "2022-05-20T13:31:26.677928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+\n",
      "|    avg(satisfied)|max(age)|\n",
      "+------------------+--------+\n",
      "|0.6886792452830188|      54|\n",
      "+------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ihre Lösung\n",
    "df.where(f.col('Department')=='HR') \\\n",
    "  .agg({'satisfied': 'mean', 'age': 'max'}) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083f5e3f",
   "metadata": {},
   "source": [
    "Wie erstellen nun ein weiteres DataFrame. Erstellen Sie aus den Daten in `dept_data` das DataFrame `departments` mit den angegebenen Spaltennamen. Geben Sie das erzeugte DataFrame aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d884ec5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:30.025682Z",
     "start_time": "2022-05-20T13:31:30.018246Z"
    }
   },
   "outputs": [],
   "source": [
    "dept_data = [(\"Sales\", \"New York\", 10001), \n",
    "            (\"HR\", \"Los Angeles\", 90001),\n",
    "            (\"Purchasing\", \"San Francisco\", 94104),\n",
    "            (\"Marketing\", \"Philadelphia\", 19110),\n",
    "            (\"Technology\", \"Los Angeles\", 90099)]\n",
    "\n",
    "column_names = [\"name\", \"location\", \"zip_code\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "84f5ef99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:31.582707Z",
     "start_time": "2022-05-20T13:31:30.647816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------+\n",
      "|      name|     location|zip_code|\n",
      "+----------+-------------+--------+\n",
      "|     Sales|     New York|   10001|\n",
      "|        HR|  Los Angeles|   90001|\n",
      "|Purchasing|San Francisco|   94104|\n",
      "| Marketing| Philadelphia|   19110|\n",
      "|Technology|  Los Angeles|   90099|\n",
      "+----------+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ihre Lösung\n",
    "departments = spark.createDataFrame(dept_data, column_names)\n",
    "departments.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbddabb",
   "metadata": {},
   "source": [
    "Wie viele Angestellte arbeiten in Los Angeles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "91683a08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:32.932839Z",
     "start_time": "2022-05-20T13:31:32.346598Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ihre Lösung\n",
    "df.join(departments, df.department == departments.name) \\\n",
    "  .where(departments.location == \"Los Angeles\") \\\n",
    "  .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2794ada0",
   "metadata": {},
   "source": [
    "Lassen Sie sich die ersten 10 Gehälter der Angestellten ausgeben, die an der Ostküste arbeiten (in diesem Fall `zip_code < 90000`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d0bffef3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T13:31:35.699842Z",
     "start_time": "2022-05-20T13:31:35.467405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| salary|\n",
      "+-------+\n",
      "|38177.1|\n",
      "|38177.1|\n",
      "|59143.5|\n",
      "|21668.4|\n",
      "|59143.5|\n",
      "|21668.4|\n",
      "|78075.0|\n",
      "|26824.5|\n",
      "|38177.1|\n",
      "|78075.0|\n",
      "+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ihre Lösung\n",
    "df.join(departments, departments.name == df.department) \\\n",
    "  .where(departments.zip_code < 90000) \\\n",
    "  .select(df.salary) \\\n",
    "  .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11623a09",
   "metadata": {},
   "source": [
    "### Caching\n",
    "Durch die In-Memory-Verarbeitung werden DataFrames in der Regel nicht zwischengespeichert. Mit jeder Action werden alle Verarbeitungsschritte neu ausgeführt. Manchmal kann es aber sinnvoll sein, ein DataFrame zwischenzuspeichern (zu *cachen*). Beim Aufruf der ersten Action wird das DataFrame dann im Speicher gehalten.\n",
    "\n",
    "(*Hinweis*: Gleiches gilt natürlich auch für RDDs, da DataFrames ja auf RDDs basieren)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbb868d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T14:08:00.667493Z",
     "start_time": "2022-05-16T14:08:00.662624Z"
    }
   },
   "source": [
    "Betrachten Sie den folgenden Code: An welcher Stelle könnte es sinnvoll sein, ein Caching durchzuführen und warum? Fügen Sie den Code für das Caching ein. Schreiben Sie in die untere Zelle Ihre Begründung und beschreiben Sie, welche DataFrames auf die gecachte Version zugreifen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819e30d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T14:08:00.667493Z",
     "start_time": "2022-05-16T14:08:00.662624Z"
    }
   },
   "source": [
    "```python\n",
    "df1 = spark.read.csv(path = \"my_data.csv\", header = True)\n",
    "df2 = df1.filter(df1.salary.between(40000, 50000))\n",
    "df3 = df2.select(\"name\", \"salary\", \"department\", \"satisfied\")\n",
    "\n",
    "# Cache df3\r\n",
    "df3.cache()\n",
    "\n",
    "df4 = df3.groupby(\"department\").agg({\"satisfied\": \"mean\"})\n",
    "df4.show()\n",
    "df5 = df3.groupby(\"department\").agg({\"salary\": \"mean\"})\n",
    "df5.show()\n",
    "df6 = df3.select(\"name\").where(df3[\"satisfied\"] == 1)\n",
    "df6.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "id": "878fb55f",
   "metadata": {},
   "source": [
    "Ihre Begründung:\n",
    "Es wäre sinnvoll, das DataFrame df3 zu cachen, da es in mehreren Operationen verwendet wird. Caching speichert das DataFrame df3 im Speicher, was den Zugriff auf die Daten beschleunigt. Da df3 in den DataFrames df4, df5 und df6 verwendet wird, kann das Caching die Gesamtlaufzeit des Programms verbessern, indem es verhindert, dass die gleichen Daten mehrmals berechnet oder abgerufen werden müssen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3558d965",
   "metadata": {},
   "source": [
    "## SQL\n",
    "Im Folgenden verwenden wir SQL zur Datenabfrage. Dafür erstellen wir zunächst eine temporale View, auf der dann die SQL-Abfragen erfolgen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6d524019",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:26:52.806431Z",
     "start_time": "2022-05-18T12:26:51.663985Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\\\n",
    "CREATE TEMP VIEW employees \\\n",
    "USING CSV \\\n",
    "OPTIONS (path = './employees_satisfaction_transformed.csv', header = 'True', inferSchema = 'True')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3219ad",
   "metadata": {},
   "source": [
    "Ermitteln Sie nun wieder die Anzahl der Angestellten pro Department, aber verwenden Sie dieses Mal SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f30abf3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:26:54.656380Z",
     "start_time": "2022-05-18T12:26:54.394996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|department|employee_count|\n",
      "+----------+--------------+\n",
      "|     Sales|            87|\n",
      "|        HR|           106|\n",
      "|Purchasing|           114|\n",
      "| Marketing|            93|\n",
      "|Technology|            98|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ihre Lösung\n",
    "spark.sql(\"\"\"\n",
    "SELECT department, COUNT(*) as employee_count\n",
    "FROM employees\n",
    "GROUP BY department\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beb4653",
   "metadata": {},
   "source": [
    "## Ressourcen-Vebrauch\n",
    "\n",
    "Sie arbeiten in diesen Übungsaufgaben mit ihrem eigenen Cluster. In der Praxis ist dies natürlich selten der Fall, hier werden meist viele Jobs gleichzeitig gestartet und konkurrieren um Ressourcen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235ba7dd",
   "metadata": {},
   "source": [
    "Öffnen Sie die __[WebUI von YARN](http://127.0.0.1:8088)__ und sehen sich die aktuellen Applikationen an. Sie sollten dort das aktuelle Jupyter Notebook finden. Solange die SparkSession (der SparkContext) erstellt ist, wird das Notebook dort als *RUNNING* aufgelistet. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dbe4f3",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "Wie viele Kerne und wie viel Speicher verbraucht die Spark-Applikation? Wie können Sie diese Summe berechnen?\n",
    "\n",
    "*Hinweis: Werfen Sie einen Blick in die Datei `spark-defaults.conf` im `SPARK_CONF_DIR`.*\n",
    "\n",
    "*Sollten Sie nicht wissen, wie Sie diese Datei finden können, finden Sie unter dem \"Show Solution\"-Button hilfreiche Kommandos.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308a9af2",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "- Über \"New\" -> \"Terminal\" können Sie ein Terminal öffnen. Alternativ können Sie in einer Code-Zelle einen Bash-Befehl ausführen, indem Sie ein ! an den Anfang der Zelle schreiben\n",
    "- Zugriff auf den Inhalt einer Umgebungs-Variablen mittels `$`, z.B. `$SPARK_CONF_DIR`\n",
    "- Anzeigen von Dateiinhalten: `cat`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b87b95f3",
   "metadata": {},
   "source": [
    "Ihre Antwort:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4953cbef",
   "metadata": {},
   "source": [
    "Stoppen Sie den SparkContext, um alle Ressourcen, die dieses Notebook benötigt, freizugeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "50743887",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-17T07:34:36.480924Z",
     "start_time": "2022-08-17T07:34:36.474478Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ihre Lösung\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba990b91",
   "metadata": {},
   "source": [
    "Öffnen Sie ein Terminal und starten Sie eine pyspark-Shell mit YARN als Ressourcen-Manager im client-mode mit 2 Exekutoren mit jeweils 1GB Memory und 2 Cores. Wie viele Cores und wie viel Speicher benötigt die Applikation und wie setzen sich diese Zahlen zusammen? Kontrollen Sie in der WebUI von YARN, ob Ihre Berechnungen korrekt sind."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a5a8b432",
   "metadata": {},
   "source": [
    "Ihre Antwort:\n",
    "\n",
    "2 Exekutoren\n",
    "Jeder Exekutor hat 1 GB Speicher\n",
    "Jeder Exekutor hat 2 Kerne\n",
    "Daher würde die gesamte Ressourcennutzung wie folgt aussehen:\n",
    "\n",
    "Cores: 1 Core AM + 2*2 Cores Executor = 5 Cores\n",
    "Memory: 896MB AM + 384MB Overhead AM + 2*(1024MB Memory Executor + 384MB Overhead\n",
    "Executor) = 4096MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8980a463",
   "metadata": {},
   "source": [
    "Starten Sie anschließend in einem zweiten Terminal eine weitere pyspark-Shell (ohne die erste Shell zu beenden) mit YARN als Ressourcen-Manager im client-mode mit der gleichen Konfiguration (2 Exekutoren mit jeweils 1GB Memory und 2 Cores). Erstellen Sie dort ein RDD mit den Werten 1, 2, 3, 4 und lassen sich dieses mittels `collect()` ausgeben.\n",
    "\n",
    "Was passiert und warum? Wie können Sie das Problem beheben?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6664d8bb-843d-4d9c-84cd-e9d8ca6328fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "cc390ab8",
   "metadata": {},
   "source": [
    "Ihre Antwort:\n",
    "Die Shell kann zwar starten und man kann auch ein RDD erstellen (Lazy Evaluation),\n",
    "allerdings kann das collect() nicht ausgeführt werden. Das liegt daran, dass nur noch\n",
    "ein Core zur Verfügung stand, welchen der Application Master bekommen hat und so die\n",
    "Shell gestartet werden konnte. Allerdings stehen jetzt für die Exekutoren keine\n",
    "Ressourcen mehr zur Verfügung. Die andere Shell muss zunächst beendet werden, damit die\n",
    "dortigen Ressourcen freigegeben werden.\n",
    "\n",
    "\n",
    "Wenn Sie versuchen, eine zweite PySpark-Shell mit der gleichen Konfiguration zu starten, während die erste noch läuft, kann es zu Ressourcenkonflikten kommen. Dies liegt daran, dass beide Shells versuchen, dieselben Ressourcen (in diesem Fall 2 Exekutoren mit jeweils 1 GB Speicher und 2 Kernen) vom YARN-Ressourcen-Manager anzufordern.\n",
    "\n",
    "Wenn Ihr Cluster nicht genügend Ressourcen hat, um beide Anforderungen zu erfüllen, kann die zweite Shell möglicherweise nicht gestartet werden oder sie kann in einen Wartezustand versetzt werden, bis die benötigten Ressourcen verfügbar sind. Dies könnte dazu führen, dass der Befehl `rdd.collect()` hängt oder eine Fehlermeldung ausgibt, weil die benötigten Ressourcen zum Ausführen der Aufgabe nicht verfügbar sind.\n",
    "\n",
    "Um das Problem zu beheben, könnten Sie eine der folgenden Optionen in Betracht ziehen:\n",
    "\n",
    "1. **Erhöhen Sie die Ressourcen Ihres Clusters**: Wenn Sie die Kontrolle über Ihren YARN-Cluster haben, könnten Sie die Anzahl der verfügbaren Kerne und/oder die Menge des verfügbaren Speichers erhöhen, um beide Shells gleichzeitig ausführen zu können.\n",
    "\n",
    "2. **Reduzieren Sie die Ressourcenanforderungen Ihrer Shells**: Sie könnten die Anzahl der Exekutoren, die Menge des Speichers pro Exekutor oder die Anzahl der Kerne pro Exekutor reduzieren, um innerhalb der verfügbaren Ressourcen Ihres Clusters zu bleiben.\n",
    "\n",
    "3. **Führen Sie die Shells nacheinander aus**: Wenn Sie nicht auf zusätzliche Ressourcen zugreifen können und Ihre Aufgaben nicht parallel ausgeführt werden müssen, könnten Sie die erste Shell beenden, bevor Sie die zweite starten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf094fe-3fa8-4d42-a82b-80888f1d953f",
   "metadata": {},
   "source": [
    "Schließen Sie beide Shells und starten anschließend eine neue pyspark-Shell mit folgenden Eigenschaften:\n",
    "- YARN als Ressourcen-Manager im client-mode\n",
    "- 2 Exekutoren mit jeweils\n",
    "    - 2GB Memory\n",
    "    - 3 Cores\n",
    "    \n",
    "Wie viele Ressourcen vebraucht diese Applikation und wie setzen sich diese zusammen?\n",
    "\n",
    "```python\n",
    "export PYSPARK_PYTHON=python3\n",
    "pyspark --master yarn --deploy-mode client --executor-memory 2g --executor-cores 3 --num-executors 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7d2f6d53-8764-4fb1-9405-49a1ed8caed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Application ID:  local-1708355894509\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# Erstellen Sie ein SparkConf-Objekt\n",
    "conf = SparkConf()\n",
    "\n",
    "conf.set('spark.executor.instances', '2')\n",
    "conf.set('spark.executor.memory', '2g')\n",
    "conf.set('spark.executor.cores', '3')\n",
    "\n",
    "spark2 = SparkSession.builder \\\n",
    "    .master('yarn') \\\n",
    "    .appName(\"My Spark Application\") \\\n",
    "    .config(conf = conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Application ID: \", spark2.sparkContext.applicationId)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d7b3eac",
   "metadata": {},
   "source": [
    "Ihre Antwort:\n",
    "2 Exekutoren\n",
    "Jeder Exekutor hat 2 GB Speicher\n",
    "Jeder Exekutor hat 3 Kerne\n",
    "Daher würde die gesamte Ressourcennutzung wie folgt aussehen:\n",
    "\n",
    "Gesamtspeicher: 2 Exekutoren * 2 GB/Exekutor = 4 GB\n",
    "Gesamtzahl der Kerne: spark.yarn.am.cores + spark.executor.instances * spark.executor.cores = 2 Exekutoren * 3 Kerne/Exekutor = 6 Kerne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200fc5a1",
   "metadata": {},
   "source": [
    "Starten Sie anschließend in einem zweiten Terminal noch eine pypsark-Shell (ohne die erste Shell zu beenden) mit folgendem Befehl:\n",
    "\n",
    "`pyspark --master yarn --conf spark.yarn.am.cores=3`\n",
    "\n",
    "Was passiert und warum? Überprüfen Sie auch den Status der Applikation in der WebUI von YARN."
   ]
  },
  {
   "cell_type": "raw",
   "id": "abaeaf3c",
   "metadata": {},
   "source": [
    "Ihre Antwort:\n",
    "Die Shell kann nicht gestartet werden, da für den AM 3 Cores angefordert werden, aber\n",
    "nur noch 2 Cores frei sind. Der Status der Applikation bleibt deshalb auf \"ACCEPTED\".\n",
    "Sie wechselt erst zu \"RUNNING\", wenn die andere Shell beendet wird und somit wieder\n",
    "genug Ressourcen zur Verfügung stehen.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
