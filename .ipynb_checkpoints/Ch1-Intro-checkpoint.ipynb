{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content\n",
    ">* [1. Introduction to Data Wrangling and Data Quality](#1)\n",
    ">* [2. Data Quality](#2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# 1. Introduction to Data Wrangling and Data Quality <a name=1></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Quality\n",
    "2. Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data wrangling\n",
    "\n",
    "- is the process of taking “raw” or “found” data, and transforming it into something that can be used to generate insight and meaning. \n",
    "\n",
    "-  it is about much more than simply learning how to access and manipulate data; it’s about making judgments, inferences, and selections.\n",
    "\n",
    "- Every significant data manipulation task is propelled by a **question**.\n",
    "\n",
    "-  the data wrangling process is really more of a **cycle** than it is a linear set of steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps of data wrangling:\n",
    "1. Researching.\n",
    "2. Locating or collecting data\n",
    "3. Reviewing the data\n",
    "4. “Cleaning,” standardizing, transforming, and/or augmenting the data\n",
    "5. Analyzing the data\n",
    "6. Visualizing the data\n",
    "7. Communicating the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Quality\n",
    "\n",
    "- It it is up to the humans involved in data collection, acquisition, and analysis to ensure its quality so that the outputs of our data work actually mean something.\n",
    "\n",
    "- axes for evaluating data quality:\n",
    "1. the **integrity** of the data itself, \n",
    "2. and the “**fit**” or appropriateness of the data with respect to a particular question or problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1.1. Data Fit\n",
    "\n",
    "-  often we have to do a significant amount of integrity work before we can know with confidence that our dataset is actually fit for our selected question or project.\n",
    "\n",
    "> **Definition** *Data Fit*\n",
    ">\n",
    "> The extent to which a given dataset accurately represents the phenomenon you're investigatig.\n",
    "\n",
    "Data fit is based on **3 metrics**: *validity*, *reliability*, and *representativeness*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **1. Validity** \n",
    "\n",
    "Describes the extent to which something measuresa what it is supposed to.\n",
    "\n",
    "\n",
    "- **Construct Validity**: This refers to how well a test or tool measures the theoretical construct that it was designed to measure¹². For example, if a test is designed to measure introversion, construct validity would be the degree to which the test actually measures introversion¹. It's especially important when researching concepts that can't be quantified and/or are intangible¹.\n",
    "\n",
    "- **Content Validity**: This assesses how well a test represents all aspects of the construct¹². If a test is designed to measure introversion, content validity would be the degree to which the test covers all aspects of introversion¹. If some aspects are missing or irrelevant parts are included, the test has low content validity¹.\n",
    "\n",
    "In summary, construct validity is about the test measuring what it's supposed to measure, while content validity is about the test covering the full breadth of the concept¹²."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Reliability\n",
    "\n",
    "Reliability of a given measure describes its accuracy and stability. Together they help us assess whether the same measure taken twice in the same circumstances will give us the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 3. Representativeness\n",
    "\n",
    "Reliability of a given measure describes whether those insights are an accurate portrait of a particular situation or population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1.2. Data integrity\n",
    "\n",
    "the integrity of a dataset is evaluated using the data values and descriptors that make it up.\n",
    "\n",
    "Data integrity is about whether the data you have can support the analysis you''ll need to perform in order to answer that quastion.\n",
    "\n",
    "> **Definition** *Data Integrity*\n",
    ">\n",
    ">Data Integrity is the completeness, accuracy, and consistency of data as it is maintained over time and across all formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data integrity is based on the following **metrics**: \n",
    "\n",
    "- ***Necessary, but not sufficient***\n",
    "    - Of known provence.\n",
    "    - Well-Annotated.\n",
    "- ***Important***\n",
    "    - Timely.\n",
    "    - Complete.\n",
    "    - High Volume.\n",
    "    - Multivariant.\n",
    "    - Atomic\n",
    "- ***Achievable***\n",
    "    - Consistent.\n",
    "    - Clear.\n",
    "    - Dimensionally structed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here are short explanations for each of the points:\n",
    "\n",
    "***Necessary, but not sufficient***\n",
    "- **Of known provenance**: This means the data's origin or source is known and can be traced. It's necessary for data integrity and authenticity, but not sufficient alone for overall data quality.\n",
    ">* _is the dataset from a reliable source?_\n",
    "- **Well-Annotated**: This refers to data being accompanied by explanatory information (annotations). While necessary for understanding the data, it alone doesn't guarantee the data's accuracy or relevance.\n",
    "\n",
    "***Important***\n",
    "- **Timely**: This means the data is available when needed. Timeliness is important for data to be actionable, but it doesn't ensure other aspects like accuracy or completeness.\n",
    ">* _is the dataset up to date?_\n",
    ">* _does the dataset include the most recent records?_\n",
    ">* _when was the last time the data was updated?_\n",
    ">* _What are the minimum and maximum dates in the table?_\n",
    "- **Complete**: Complete data has all the necessary parts. It's important for a comprehensive analysis, but doesn't ensure the data is timely or accurate.\n",
    "> _important questions would be:_\n",
    ">* _Are there any missing values in the data?_\n",
    ">* _Find the earliest date(s) in our “recent” data file and confirm that they are before a specific date._\n",
    "- **High Volume**: This refers to having a large amount of data, which is important for statistical significance. However, high volume doesn't ensure the data is relevant or accurate.\n",
    ">Is the number of data in your dataset is enough?\n",
    "- **Multivariant**: This means the data covers multiple variables or factors. It's important for a holistic view, but doesn't ensure the data is complete or timely.\n",
    "- **Atomic**: This refers to data that is in its smallest indivisible unit, providing a high level of detail. It's important for granular analysis, but doesn't ensure the data is complete or timely.\n",
    "\n",
    "***Achievable***\n",
    "- **Consistent**: This means the data is uniform and reliable across all instances. It's achievable with good data governance, but doesn't ensure the data is timely or complete.\n",
    "> _important questions would be:_\n",
    ">* _are there duplicates?_\n",
    ">* _Do the data match when thy are read from two different sources?_\n",
    ">* are the decriptives used for the same value in the dataset consistent? (e.g. spelling of \"male\" vs \"Male\")\n",
    "- **Clear**: This refers to data that is easy to understand and interpret. It's achievable with good data presentation, but doesn't ensure the data is accurate or complete.\n",
    "- **Dimensionally Structured**: This means the data is organized in a way that allows analysis across different dimensions (e.g., time, location). It's achievable with good data modeling, but doesn't ensure the data is timely or accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. RDDs <a name=2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RDDs bilden diese die Grundlage der parallelen Verarbeitung.\n",
    "- die wichtigsten Eigenschaften von RDDs:\n",
    "    - In-Memory-Verarbeitung\n",
    "    - Lazy\n",
    "    - Partitioniert\n",
    "    - Unveränderlich\n",
    "    - Lineage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Voretile der In-Memory-Verarbeitung**\n",
    "- *Schnellere Datenverarbeitung*: Durch die Speicherung von Daten im Arbeitsspeicher (In-Memory) können Datenverarbeitungsaufgaben schneller ausgeführt werden als bei herkömmlichen Festplatten-basierten Systemen wie Hadoop.\n",
    "\n",
    "- *Effeziente Wiederverwendung von Daten*: viele Algorithmen und Abfragen arbeiten iterativ auf demselben Datensatz. Die In-Memory-Verarbeitung ermöglicht eine effiziente Wiederverwendung dieser Daten, da sie zwischen Abfragen im Arbeitsspeicher gespeichert werden können.\n",
    "- *Fehlertoleranz*: RDDs können nach einem Ausfall wiederhergestellt werden. Dies wird erreicht, indem die Abstammungslinie (Lineage) jedes RDDs verfolgt wird.\n",
    "- *Skalierbarkeit und Prallelität*: RDDs können über ein Cluster verteilt und parallel verarbeitet werden, was die Verarbeitung von extrem großen Datenmengen ermöglicht\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lazy Verarbeitung**\n",
    "\n",
    "In der Programmierung bedeutet “Lazy-Ausführung” (oder “Lazy-Evaluation”), dass Berechnungen erst dann ausgeführt werden, wenn sie tatsächlich benötigt werden. Dies kann die Effizienz von Programmen verbessern, indem unnötige Berechnungen vermieden und Ressourcen gespart werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformations und Actions:**\n",
    "\n",
    "In Apache Spark sind Transformationen und Aktionen zwei Arten von Operationen, die auf Daten angewendet werden können. Hier sind die wichtigsten Unterschiede:\n",
    "\n",
    "- **Transformationen** erstellen ein neues Dataset aus einem vorhandenen Dataset. Beispiele für Transformationen sind `map`, `filter` und `reduceByKey`. Transformationen in Spark sind \"lazy\", was bedeutet, dass sie erst ausgeführt werden, wenn eine Aktion aufgerufen wird. Sie führen also keine Berechnungen durch, wenn sie definiert werden, sondern erstellen lediglich einen Ausführungsplan.\n",
    "\n",
    "- **Aktionen** hingegen liefern einen Wert an den Treiberprogramm zurück oder schreiben Daten in ein externes Speichersystem. Aktionen lösen die tatsächliche Berechnung aus, die durch die Transformationen definiert wurde. Beispiele für Aktionen sind `count`, `first`, `take`, `collect` und `save`.\n",
    "\n",
    "Zusammengefasst, Transformationen erstellen einen Ausführungsplan, aber berechnen nichts, bis eine Aktion aufgerufen wird. Aktionen lösen die Berechnung aus und geben das Ergebnis zurück. Dieses Konzept ermöglicht es Spark, seine Berechnungen zu optimieren und effizient über große Datenmengen hinweg durchzuführen.\n",
    "\n",
    "\r\n",
    "|  | Transformationen | Aktionen |\r\n",
    "|---|---|---|\r\n",
    "| **Was sie tun** | Erstellen ein neues Dataset aus einem vorhandenen Dataset | Liefern einen Wert an das Treiberprogramm zurück oder schreiben Daten in ein externes Speichersystem |\r\n",
    "| **Beispiele** | `map`, `filter`, `reduceByKey` | `count`, `first`, `take`, `collect`, `save` |\r\n",
    "| **Wann sie ausgeführt werden** | Sie sind \"lazy\" und werden erst ausgeführt, wenn eine Aktion aufgerufen wird | Sie lösen die tatsächliche Berechnung aus, die durch die Transformationen definiert wurde |\r\n",
    "| **Ergebnis** | Sie erstellen einen Ausführungsplan, berechnen aber nichts | Sie lösen die Berechnung aus und geben das Ergebnis zuröglichen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Leere Partitionen**\n",
    "\n",
    "Leere Partitionen in einem RDD können zu mehreren Problemen führen:\n",
    "\n",
    "1. **Ressourcenverschwendung**: Jede Partition wird von einem separaten Task verarbeitet. Wenn eine Partition leer ist, wird ein Task verschwendet, da es nichts zu verarbeiten gibt.\n",
    "2. **Ungleichmäßige Lastverteilung**: Wenn einige Partitionen viele Daten enthalten und andere leer sind, kann dies zu einer ungleichmäßigen Lastverteilung führen. Einige Tasks können sehr lange dauern, während andere schnell abgeschlossen sind.\n",
    "\n",
    "$\\longrightarrow$ Um das Problem leerer Partitionen zu lösen, kann man das RDD neu partitionieren, um die Daten gleichmäßiger zu verteilen. Dieser Prozess wird als **Shuffling** bezeichnet. Mit der `repartition()` Funktion in Spark kann man die Anzahl der Partitionen in einem RDD ändern:\n",
    "\n",
    "```python\n",
    "rdd_repartitioned = rdd_filtered.repartition(10)\n",
    "```\n",
    "\n",
    "Obwohl Shuffling das Problem leerer Partitionen lösen kann, sollte es normalerweise vermieden werden, da es mit hohen Kosten verbunden ist:\n",
    "\n",
    "1. **Hoher Netzwerkverkehr**: Beim Shuffling werden alle Daten über das Netzwerk gesendet, was zu hohem Netzwerkverkehr führen kann.\n",
    "2. **Hoher Disk I/O**: Die Daten müssen auf die Festplatte geschrieben und von dort gelesen werden, was zu hohem Disk I/O führt.\n",
    "3. **Hoher CPU-Verbrauch**: Das Sortieren und Aggregieren der Daten erfordert CPU-Ressourcen.\n",
    "\n",
    "Daher sollte Shuffling nur dann verwendet werden, wenn es unbedingt notwendig ist, z.B. wenn die Vorteile der Neuverteilung der Daten die Kosten des Shufflings überwiegen. Es ist immer eine gute Praxis, die Daten so früh wie möglich im Verarbeitungsprozess zu partitionieren, um die Notwendigkeit von Shuffling zu minimieren.\n",
    "\n",
    "1. **Ressourcenverschwendung**: Jede Partition wird von einem separaten Task verarbeitet. Wenn eine Partition leer ist, wird ein Task verschwendet, da es nichts zu verarbeiten gibt.\n",
    "2. **Ungleichmäßige Lastverteilung**: Wenn einige Partitionen viele Daten enthalten und andere leer sind, kann dies zu einer ungleichmäßigen Lastverteilung führen. Einige Tasks können sehr lange dauern, während andere schnell abgeschlossen sind.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unveränderlich und Lineage**\n",
    "Resilient Distributed Datasets (RDDs) sind unveränderlich, was bedeutet, dass sie nach ihrer Erstellung nicht mehr verändert werden können. Dies hat mehrere Vorteile:\n",
    "\n",
    "1. **Fehlerbehandlung**: Da RDDs unveränderlich sind, können sie bei einem Fehler einfach neu erstellt werden. Dies macht das System widerstandsfähiger gegen Ausfälle.\n",
    "2. **Parallelisierung**: Unveränderlichkeit erleichtert die Parallelisierung von Operationen, da es keine Synchronisationsprobleme gibt, die normalerweise bei veränderlichen Daten auftreten.\n",
    "3. **Effizienz**: Da RDDs unveränderlich sind, können sie effizient auf mehreren Knoten verteilt und zwischengespeichert werden, was die Verarbeitungsgeschwindigkeit verbessert.\n",
    "\n",
    "Jedes RDD kennt seine Abstammung oder \"Lineage\", was bedeutet, dass es die Reihe von Transformationen kennt, die zu seiner Erstellung geführt haben. Dies hat auch mehrere Vorteile:\n",
    "\n",
    "1. **Fehlerbehandlung**: Wenn ein RDD verloren geht, kann es anhand seiner Lineage neu erstellt werden. Dies macht das System widerstandsfähiger gegen Ausfälle.\n",
    "2. **Optimierung**: Die Kenntnis der Lineage ermöglicht es Spark, die Verarbeitung zu optimieren. Zum Beispiel kann Spark entscheiden, einige Operationen zusammenzufassen oder die Reihenfolge der Operationen zu ändern, um die Effizienz zu verbessern.\n",
    "\n",
    "Zusammenfassend lässt sich sagen, dass die Unveränderlichkeit und die Kenntnis der Lineage zwei der Schlüsseleigenschaften von RDDs sind, die Spark seine Robustheit und Effizienz verleihen. Sie ermöglichen eine effiziente verteilte Verarbeitung und eine robuste Fehlerbehandlung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
